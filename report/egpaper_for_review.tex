\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{subcaption} 
\usepackage{float}
\usepackage[boxed]{algorithm2e}
\usepackage{algorithmic}
\usepackage{color}
\definecolor{darkred}{rgb}{0.8,0,0}
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkblue}{rgb}{0,0,0.6}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% TODO: Uncomment this line to enable the RULER, which I think we need for the final submission? Verify this.
\cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
% \ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{RetroGAN: Translating Unpaired Video Game Images Using CycleGANs}

\author{Andrew Rollings, Michael Townsend, Tyler Thurston\\
Georgia Institute of Technology\\
% NOTE: Do we include email?
{\tt\small arollings3@gatech.edu, mtownsend31@gatech.edu, tthurston7@gatech.edu}
}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   Video games from the retro era have a signature style due to the 8-bit and 16-bit technology at the time. Limitations on resolution, color, bandwidth, and processing power all created constraints that limited the vision and scope of what video game creators could implement. Using a CycleGAN architecture to allow unpaired translation, we can learn from screenshot data of two separate video game generations (8-bit and 16-bit) and translate images from one generation to the other. Our results show that we can take images from the Nintendo Entertainment System (NES) and Super Nintendo (SNES) and translate screenshots from both consoles into the style of the other. Experiments demonstrate that the CycleGAN architecture can learn many constraints of each system in order to perform relatively accurate translations between generations.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction/Background/Motivation}
\textit{\textcolor{blue}{(5 points) What did you try to do? What problem did you try to solve? Articulate your objectives using absolutely no jargon.}}

In the 1980s, 8-bit consoles were king. These were characterized by limited color palettes and relatively simple 2D graphics. In the 1990s, 16-bit consoles took over and offered increased power, color palettes and the ability to display more complex graphics.\\
Despite huge advances in technology, retro-games from the 8- and 16-bit eras remain popular, as evidenced by the number of remakes, emulators, modern ``Indie'' games using pixel art and even specialist magazines dedicating to retro gaming.\\
Our experiment is based on the observation that the primary difference between 8- and 16-bit games (as exemplified by the NES and SNES, respectively) was in the fidelity of the graphics. Additionally, many of the graphical remasters today view the 16-bit era as the gold standard in pixel-based artwork, given that the graphical capabilities of the SNES still allowed artists to express themselves far more than the limited NES.\\
Our primary objective is to train two systems, one that can take in an 8-bit NES graphic and ``up-convert'' it to a realistic 16-bit version and another system that can take in a 16-bit SNES graphic and ``down-convert'' it to a realistic 8-bit version.

\textit{\textcolor{blue}{(5 points) How is it done today, and what are the limits of current practice?}}

Another area of interest is in software emulators that are often used to breathe life into older games. For example, the Nintendo Switch provides a paid emulation service that allows players to access a library of older games. In some cases, the emulators provide graphical filters that allow the appearance of the game to be altered (e.g. a CRT filter, or a resolution upscaling filter). However, currently the only way to improve the graphical color depth of a game is for artists to redraw all of the game's graphics.

\textit{\textcolor{blue}{(5 points) Who cares? If you are successful, what difference will it make?}}

If successful, this can allow the graphical content of a particular video game console to be translated into the style of another. Pixel artists are in very high demand and creating high quality pixel art is one of the largest financial strains that can be put on games, and is very time consuming/labor intensive. Pixel artists are often so hard to find that studios will save money by switching to 3D modeling as there are more artists and tools to assist them. This could allow for pixel artists to become more efficient, taking images from a previous game or console generation and translating it to the current project's generation. For instance, if one wished to modernize an 8-bit game, retroGAN enables someone to take the images from the original game and create new baseline screenshots for the sequel. Pixel artists would only need to tweak the translation instead of starting from scratch. There is also the interesting possibility of creating a real-time filter that could do this conversion ``on-the-fly'' as the game is being played. Additionally, this conversion process is not constrained to translation between NES and SNES, and could be done on other consoles and generations given suitable datasets.

\textit{\textcolor{blue}{(5 points) What data did you use? Provide details about your data, specifically choose the most important aspects of your data mentioned \href{https://arxiv.org/abs/1803.09010}{here}. You donâ€™t have to choose all of them, just the most relevant.}}

Datasets for NES/SNES images did not exist in a suitable form, so we created our own from various screenshot databases on the internet. We implemented our own pre-processing, which included unsupervised color clustering to remove image artifacts, clamping colors to more closely adhere to the console's original color palette, resizing and cropping as appropriate, and other noise reduction techniques. This provided us with a reasonably large set of images that were organized into quality bands based on how much preprocessing was required. We cover this in more detail later in the paper.

%-------------------------------------------------------------------------
%------------------------------------------------------------------------
\section{Approach}

\textit{\textcolor{blue}{(10 points) What did you do exactly? How did you solve the problem? Why did you think it would be successful? Is anything new in your approach?}}

We attempted to implement translation between video game images from different generations. The primary problem was a lack of paired image data (an image from each generation) since each console generation consists almost solely of different games (with a small number of remakes). We thought CycleGAN had the potential to be a solution as it doesn't require paired data due to the cyclic process for training. That is, a NES image can be converted to a SNES fake by one leg of the cycle, and then the SNES fake can be converted back to a NES ``fake fake'' by the reverse leg. Then the round-trip images (the NES original and its doubly-converted ``fake fake'') can be compared for differences. In an ideal system, the real image and the ``fake fake'' image would be virtually identical. Both directions of the conversion process (NES$\rightarrow$SNES and SNES$\rightarrow$NES) can be cross-checked in this fashion.

We initially started by taking the driver code\footnote{https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix, Commit Id 00d5574908eb66fe0127b32d7b030001453f21d0} for the original CycleGAN paper \cite{CycleGAN}. We then removed all of the code not directly related to the CycleGAN model and added our own datasets, pre-processing, augmentation, hyper-parameter tuning code, and metrics.

Given the almost miraculous results in the literature that have been achieved using CycleGANs with unpaired datasets (and admittedly vast computing resources way beyond our reach), we were reasonably confident that we could use similar techniques to perform a non-trivial conversion between 8-bit and 16-bit screenshots of games. We expected the 16$\rightarrow$8-bit conversion to be much simpler than the 8$\rightarrow$16-bit conversion, but we were hopeful that we might be able to achieve at least some worthwhile results in the latter case.

Our approach using the existing CycleGAN architecture with our custom dataset is unique and to our knowledge, no one has properly explored this type of translation.

\textit{\textcolor{blue}{(5 points) What problems did you anticipate? What problems did you encounter? Did the very first thing you tried work?}}

The main problem we expected was a lack of clean data. While there are plenty of screenshots of games available across the Internet, we could not find any pre-sorted collections of clean images. As such, we ended up writing several custom web scrapers and image processing tools to collect and homogenize screenshots from several game catalog websites. This process is covered in more detail later in the paper.

Additionally, we had concerns that the CycleGAN would be able to capture the ``pixel-perfect'' constraints and requirements for the conversion process. Previously, CycleGANs have been typically used for experiments that have not required such exact results; for example, the well-known Horse to Zebra results were not conditioned on the exact shades of white and black of the zebra stripes.

Conversely, in our situation, there are several palette restrictions that apply to the NES and SNES consoles. While the main difference between these consoles is their available color palettes, the NES also has several other graphical restrictions that place some difficult to quantify restrictions on the number of colors in a particular areas of the screen. The SNES also has similar restrictions, although nowhere near as severe. This was anticipated to be very difficult to generalize without magnitudes more data that we had available to us (approximately 100,000 images of varying quality between the two consoles). We didn't see great results in our first attempt, however after performing some additional pre-processing (covered later in this paper), we started seeing interesting results with convincing translations very early on.

\textbf{\textcolor{blue}{Important: Mention any code repositories (with citations) or other sources that you used, and specifically what changes you made to them for your project.}}

\subsection{NES / SNES Palette Considerations}
The NES palette comprises 56 fixed colors\footnote{https://en.wikipedia.org/wiki/List\_of\_video\_game\_console\_palettes}, whereas the SNES is more flexible with a full 15-bit RGB palette (with 5 bits for each of the red, green and blue channels)\footnote{https://en.wikipedia.org/wiki/List\_of\_monochrome\_and\_RGB\_color\_formats}, as shown in Figures \ref{fig:nes} and \ref{fig:snes} respectively.

As previously mentioned, there are some additional restrictions to how these colors can be used, but we do not cover them here; they are of more concern to emulator writers, and it was hoped that \--- given enough data \--- the CycleGAN would be able to figure these out for itself.

As such, for an effective conversion, the GAN must learn to be conform to each palette. In the case of SNES${\rightarrow}$NES, this could be as simple as a naive nearest color reduction from the 15-bit SNES palette to the fixed NES palette. Similarly, for the NES${\rightarrow}$SNES conversion, the GAN could simply ensure that the chosen colors fit the 15-bit RGB restriction. However, we hoped for (and observed) more complex behaviors than this, implying that the GAN was able to somewhat successfully extract more of the stylistic differences between NES and SNES graphics. This will be discussed in later sections.

\begin{figure}[htp]
   \centering
   \begin{subfigure}[b]{0.235\textwidth}
      \includegraphics[width=\textwidth]{figures/NES_palette_color_test_chart.png}
      \caption{NES 56 Color Palette}
      \label{fig:nes}
   \end{subfigure}
   \begin{subfigure}[b]{0.235\textwidth}
      \includegraphics[width=\textwidth]{figures/SNES_palette_color_test_chart.png}
      \caption{SNES 15-bit Palette}
      \label{fig:snes}
   \end{subfigure}
   \caption{Color palettes for NES and SNES}
\end{figure}

\subsection{Generative Models and CycleGANs}
For our main architecture, we chose CycleGAN for the many positives it brings. First, being a Generative Model, it brings us the ability to generate new models via sampling from our implicit density estimation. On top of that, we also were drawn by the advantages of specifically Generative Adversarial Networks(GANs)\cite{GAN}. GANs provide several benefits over other generative models, including not needing labeled data as GANs are unsupervised. They are also flexible on the types of data they ingest. Most importantly, since they sample from their density instead of doing any kind of averaging, GANs result in the sharpest images over other generative models. This is a very important property as retro graphics involve small, sharp pixel placement which makes this a vital quality for this dataset.

Finally, we chose CycleGAN\cite{CycleGAN}, as it provides the ability to translate between two datasets without the need for paired images. Instead of the usual GAN architecture of a single generator and discriminator, the CycleGAN architecture uses mappings: $G$ and $F$ with the goal of having $G$ learn a mapping from domain $X \rightarrow Y$ such that $G(X)$ is indistinguishable from the distribution $Y$ using adversarial loss. However, due to the low constraints of unpaired data, a second, inverse mapping $F: Y \rightarrow X$ is created to induce a cycle consistency loss to push $F(G(X)) \approx X$, as shown:
\begin{equation*}
   G: X \rightarrow Y, F: Y \rightarrow X : F(G(X)) \approx X
\end{equation*}

These mappings are combined with two \textit{cycle consistency losses} that can capture the intuition that translating from one domain to another and back again should result to something similar to the original input.

Additionally, an \textit{identity loss} is used to provide color/hue stability. This is defined as $F(X) \approx X$ and $G(Y) \approx Y$. The \textit{identity loss} is combined with the \textit{cycle consistency loss} proportionally as determined by a bias hyperparameter.


\begin{figure}[H]
   \centering
   \includegraphics[width=0.485\textwidth]{figures/CycleGANChart.png}
   \caption{CycleGAN Architecture}
\end{figure}

\textcolor{red}{*** NEED A LITTLE BIT ABOUT RESNET AND UNET HERE?}

\section{Experiments and Results}

\textit{\textcolor{blue}{(10 points) How did you measure success?}}

The CycleGAN framework allowed for significant configurability of hyperparameters, and we took full advantage of the flexibility provided. As such, for our first pass success measure, we selected for the hyperparameters that showed the smallest amount of loss for at least \textit{one} direction of the cycle. This gave us a shortlist of candidates to examine that we evaluated manually to determine which to investigate further. One interesting (but not entirely unexpected) observation we found was that the cycle performance was not even; a set of hyperparameters that performed well for the NES$\rightarrow$SNES conversion would not necessarily perform well for the SNES$\rightarrow$NES conversion, and vice versa. 

For each promising candidate set, we then ran a customized metric to evaluate the final images for their adherence to the hardware restrictions of each console. We produced several iterations of this metric, before finally settling on Algorithm \ref{alg:alg1}. 

This metric scores an image based on how closely the pixels match the target console palette modulated by the percentage of image pixels that match the target palette exactly. This modulation was necessary to compensate for the fact that the SNES palette has many more entries, and consequently, it was easier for the algorithm to score highly as any selected color was never too far away from a valid palette entry in RGB space. Although this metric does not capture 100\% of the intricacies of the NES and SNES screen palette restrictions, it does cover most situations that would occur. In any case, the ability to detect the outlier violations would require intrinsic knowledge of the programming of the particular game that had generated the screenshot, which is far beyond the scope of this paper.

In practice, we found that using the stricter NES metric produced more meaningful results; that is, for the SNES$\rightarrow$NES conversion, we found that measuring the increase in value for this metric between the real SNES and fake NES image was a strong indicator of success. Conversely, for the NES$\rightarrow$SNES conversion, the decrease in value of this metric was a good indicator of success.

\textcolor{darkgreen}{Note that this was applied as a post-training evaluation metric. An idea for future work would be to implement this metric as part of the loss function; unfortunately, time constraints prevented us from performing this investigation ourselves.}

\textit{\textcolor{blue}{What experiments were used?}}

Using the hyperparameter search driver, each of the three authors ran a random hyperparameter search using the time available; once this was complete we first compared our results by the metric described above, selecting a shortlist of best candidates. We considered each direction of the cycle separately for the reasons mentioned previously. For each candidate, we performed a visual inspection of the output and reached a consensus on which of these had generated the best overall results. The final candidate from each cycle direction was then used to generate the images and graphs in the following sections.

\textit{\textcolor{blue}{What were the results, both quantitative and qualitative? Did you succeed? Did you fail? Why? Justify your reasons with arguments supported by evidence and data.}}

\subsection{SNES$\rightarrow$NES Results}

As expected, the SNES$\rightarrow$NES conversion proved to be the most effective cycle leg. For a training group size of 2500, the averaged NES metric was $0.9848$, indicating that across all of the training images, roughly 98\% of the pixels conformed to the NES palette.


\textcolor{red}{***HYPERPARAMETERS HERE}

We selected several of the most aesthetically pleasing conversion images, and re-ran the NES metric on both the real and fake images, as shown in table \ref{tab:nesresults}.
These results show that, while there was definite improvement, and hence learning, the network was unable to score the same metric as the training set. For a dataset size of 2500 (with a 90/10 train/test split), the averaged SNES metric was $0.6382$, indicating that across all of the training images, roughly 64\% of the pixels conformed to the NES palette. The specific dataset size was chosen due to training time constraints.

Despite the lower metric score, a visual examination of the results in figures \ref{fig:ss1a} and \ref{fig:ss1b} show that the fake image is clearly closer to the target aesthetic, and the conversion process appears to be more involved than a simple palette conversion.
In particular, figure \ref{fig:ss1b} seems to show that different conversion criteria have been applied to different areas of the image. If one compares the lower left brighter green area in figure \ref{fig:ss1a} with the similar intensity green area just right of center, it appears that the lower left area is rendered brighter in \ref{fig:ss1b} than the right of center area, even though the same areas in the source image are of similar intensity.

\textcolor{red}{*** OVERFITTING? NOT ENOUGH DATA?}

\begin{table}
   \begin{center}
      \begin{tabular}{|l|c|c|}
         \hline
         Source SNES Image & Real  & Fake           \\
         \hline\hline
         Example 1 & $0.0449$  & $0.5190$           \\
         Example 2  & $0.0171$ & $0.3330$           \\
         \hline
      \end{tabular}
   \end{center}
   \caption{SNES$\rightarrow$NES Results (NES Metric)}
   \label{tab:nesresults}
\end{table}
       

\textbf{\textcolor{blue}{Important: This section should be rigorous and thorough. Present detailed information about decision you made, why you made them, and any evidence/experimentation to back them up. This is especially true if you leveraged existing architectures, pre-trained models, and code (i.e. do not just show results of fine-tuning a pre-trained model without any analysis, claims/evidence, and conclusions, as that tends to not make a strong project). }}

%% screenshot 1
\begin{figure}[htp]
   \centering
   \begin{subfigure}[b]{0.225\textwidth}
      \includegraphics[width=\textwidth]{figures/snes_to_nes/AV_Mahjong_Club_(J)_(Unl)_copy__ucc__8_real_B.png}
      \caption{Real SNES Screenshot}
      \label{fig:ss1a}
   \end{subfigure}
   \begin{subfigure}[b]{0.225\textwidth}
      \includegraphics[width=\textwidth]{figures/snes_to_nes/AV_Mahjong_Club_(J)_(Unl)_copy__ucc__8_fake_A.png}
      \caption{Fake NES screenshot}
      \label{fig:ss1b}
   \end{subfigure}
   \caption{Example 1: SNES$\rightarrow$NES}
\end{figure}

%% screenshot 2
\begin{figure}[htp]
   \centering
   \begin{subfigure}[b]{0.235\textwidth}
      \includegraphics[width=\textwidth]{figures/snes_to_nes/ghouls_n_ghosts_real_B.png}
      \caption{Real SNES Screenshot}
      \label{fig:ss2a}
   \end{subfigure}
   \begin{subfigure}[b]{0.235\textwidth}
      \includegraphics[width=\textwidth]{figures/snes_to_nes/ghouls_n_ghosts_fake_a.png}
      \caption{Fake NES screenshot}
      \label{fig:ss2b}
   \end{subfigure}
   \caption{Example 2: SNES$\rightarrow$NES}
\end{figure}

\subsection{NES$\rightarrow$SNES Results}

For the NES$\rightarrow$SNES conversion we performed an almost identical analysis. We selected several of the most aesthetically pleasing conversion images, and re-ran the stricter NES metric on both the real and fake images, as shown in table \ref{tab:snesresults}. 

\textcolor{red}{***HYPERPARAMETERS HERE}

Although it is apparent that this leg of the conversion was not as successful as the reverse leg, it is still notable that the results show that some selective conversion has taken place. Notably, the results show that background areas tended to be lightened and blurred, bright spots tended to be blown out and made brighter, while areas of darker colors \--- particular those with black pixel outlines \--- tended to be left relatively untouched, implying that the network did at least extract some relevant feature information. 

\textcolor{red}{*** UNDERFITTING? NOT ENOUGH DATA?}

\begin{table}
   \begin{center}
      \begin{tabular}{|l|c|c|}
         \hline
         Source NES Image & Real  & Fake           \\
         \hline\hline
         Example 1 & $1.0$  & $0.0155$           \\
         Example 2  & $1.0$ & $0.0157$           \\
         \hline
      \end{tabular}
   \end{center}
   \caption{NES$\rightarrow$SNES Results (NES Metric)}
   \label{tab:snesresults1}
\end{table}


%% screenshot 3
\begin{figure}[htp]
   \centering
   \begin{subfigure}[b]{0.235\textwidth}
      \includegraphics[width=\textwidth]{figures/nes_to_snes/Crisis_Force_(J)__ucc__12_real_A.png}
      \caption{Real NES Screenshot}
      \label{fig:ss3a}
   \end{subfigure}
   \begin{subfigure}[b]{0.235\textwidth}
      \includegraphics[width=\textwidth]{figures/nes_to_snes/Crisis_Force_(J)__ucc__12_fake_B.png}
      \caption{Fake SNES screenshot}
      \label{fig:ss3b}
   \end{subfigure}
   \caption{Example 3: NES$\rightarrow$SNES}
\end{figure}

%% screenshot 4
\begin{figure}[htp]
   \centering
   \begin{subfigure}[b]{0.235\textwidth}
      \includegraphics[width=\textwidth]{figures/nes_to_snes/Ninja_Kun_-_Majou_no_Bouken_(J)__ucc__8_real_A.png}
      \caption{Real NES Screenshot}
      \label{fig:ss4a}
   \end{subfigure}
   \begin{subfigure}[b]{0.235\textwidth}
      \includegraphics[width=\textwidth]{figures/nes_to_snes/Ninja_Kun_-_Majou_no_Bouken_(J)__ucc__8_fake_B.png	}
      \caption{Fake SNES screenshot}
      \label{fig:ss4b}
   \end{subfigure}
   \caption{Example 4: NES$\rightarrow$SNES}
\end{figure}


%%%%%%%%% CONCLUSION
\section{Conclusion}

The golden age of 8-bit and 16-bit pixel art is past but the need for generating this type of graphic is ever-present. To this end we developed retroGAN, an implementation of CycleGAN. Its purpose is to up-convert NES to SNES images and down-convert SNES to NES images.

To train the CycleGAN, we generated an unpaired dataset of NES and SNES screenshots. Multiple layers of image preprocessing and augmenting were implemented to improve its learning. The loss function was adequate to learn the image conversion; however, we needed to know how well it was learning the correct color palette as well, which forced us to engineer a unique scoring metric.

As you can see from the results, retroGAN is quite effective at its job. Follow up work should include using our scoring metric as a loss function or scaling images up to 32 or 64-bit systems. 

%-------------------------------------------------------------------------
\section{Other Sections}

\textit{\textcolor{blue}{You are welcome to introduce additional sections or subsections, if required, to address the following questions in detail.}}

\textit{\textcolor{blue}{(5 points) Appropriate use of figures / tables / visualizations. Are the ideas presented with appropriate illustration? Are the results presented clearly; are the important differences illustrated?}}

This seems like free points, just properly caption tables and figures. We just need to make sure when we're showing before-after images to make sure we show the metric improving and talk about it in analysis.

\textit{\textcolor{blue}{(5 points) Overall clarity. Is the manuscript self-contained? Can a peer who has also taken Deep Learning understand all of the points addressed above? Is sufficient detail provided?}}

This seems like free points. We probably just need to briefly tough on GANs and give a proper section on how CycleGANs work, as well as some detail between NES and SNES color palettes.

\textit{\textcolor{blue}{(5 points) Finally, points will be distributed based on your understanding of how your project relates to Deep Learning. Here are some questions to think about:}}

This also seems free. We should probably go into the architecture of each GAN just so we can estimate how many parameters we have. (should probably wait until final HPs are done)

\textit{\textcolor{blue}{What was the structure of your problem? How did the structure of your model reflect the structure of your problem?}}

Our problem had two main pieces, a set of NES images and a set of SNES images, without a successful way to map from one domain to the other. The CycleGAN model fit our needs perfectly. Since it is composed of two GANs and two discriminators, it learned a mapping between domains.

\textit{\textcolor{blue}{What parts of your model had learned parameters (e.g., convolution layers) and what parts did not (e.g., post-processing classifier probabilities into decisions)? }}

Both GANs and discriminators had learned parameters. The model trained one GAN to make NES images and the other to make SNES images while the two discriminators were trained to detect real and fake NES and SNES images, respectively.

\textit{\textcolor{blue}{What representations of input and output did the neural network expect? How was the data pre/post-processed?}}

This is \textcolor{red}{Andrew}'s section. Should talk about the pre-processing we did.

\textit{\textcolor{blue}{What was the loss function?}}

TODO: Depends on best one picked via hyperparameter. We might want to mention the losers in passing.

\textit{\textcolor{blue}{Did the model overfit? How well did the approach generalize?}}

This is probably a good way to talk about our augmentation and how it helps with over-fitting.

\textit{\textcolor{blue}{What hyperparameters did the model have? How were they chosen? How did they affect performance? What optimizer was used?}}

The original CycleGAN architecture did not have a method for finding the best hyperparameters. We built a wrapper around the train function that allowed us to randomize the available hyperparameters and run multiple training iterations, evaluating each one and retaining the best performing.

TODO: Add in the hyperparameters we ended up using.

\textit{\textcolor{blue}{What Deep Learning framework did you use?}}

PyTorch

\textit{\textcolor{blue}{What existing code or models did you start with and what did those starting points provide?}}

We used a CycleGAN framework, which is composed of two GANs and two discriminators. The first GAN generates an image, which is passed to the first discriminator, making a prediction on whether the image received is real or fake. The image is then passed to a second GAN, which tries to remake the original image. The output is then passed to a second discriminator, which evaluates whether it received a real or fake image.

The framework provided a first pass implementation for us, giving time to align the framework to our specific problem.

\textit{\textcolor{blue}{Briefly discuss potential future work that the research community could focus on to make improvements in the direction of your project's topic.}}

This is an interesting one, we could talk about using sprites, more data, other consoles, using the metrics we made with loss, all sorts of stuff.

Additional work could focus on expanding the consoles that the framework could work on. For example, mapping from 16-bit to 32-bit or even mapping larger jumps from 16-bit to 64-bit. There would be added complexity to this, which may result in needing larger training sets or alternate loss functions.

Alternate loss functions in particular are an intersting area for further research. The CycleGAN architecture was built for the general problem on mapping images from one domain to another. However, for the specific problem of mapping images from one console generation to another, loss functions could be more precise. Others could research using the loss function to additionally model the console's graphical constraints, learning the exact color palette of each console generation.

%-------------------------------------------------------------------------

\section{Work Division}

\textit{\textcolor{blue}{Please add a section on the delegation of work among team members at the end of the report, in the form of a table and paragraph description. This and references do \textbf{NOT} count towards your page limit. An example has been provided in Table \ref{tab:contributions}.}}

\newpage
\newpage
\section{Miscellaneous Information}

The rest of the information in this format template has been adapted from CVPR 2020 and provides guidelines on the lower-level specifications regarding the paper's format.

\subsection{Language}

All manuscripts must be in English.


\subsection{Paper length}
Papers, excluding the references section,
must be no longer than six pages in length. The references section
will not be included in the page count, and there is no limit on the
length of the references section. For example, a paper of six pages
with two pages of references would have a total length of 8 pages.

%-------------------------------------------------------------------------
\subsection{The ruler}
The \LaTeX\ style defines a printed ruler which should be present in the
version submitted for review.  The ruler is provided in order that
reviewers may comment on particular lines in the paper without
circumlocution.  If you are preparing a document using a non-\LaTeX\
document preparation system, please arrange for an equivalent ruler to
appear on the final output pages.  The presence or absence of the ruler
should not change the appearance of any other content on the page.  The
camera ready copy should not contain a ruler. (\LaTeX\ users may uncomment
the \verb'\cvprfinalcopy' command in the document preamble.)  Reviewers:
note that the ruler measurements do not align well with lines in the paper
--- this turns out to be very difficult to do well when the paper contains
many figures and equations, and, when done, looks ugly.  Just use fractional
references (e.g.\ this line is $095.5$), although in most cases one would
expect that the approximate location will be adequate.

\subsection{Mathematics}

Please number all of your sections and displayed equations.  It is
important for readers to be able to refer to any particular equation.  Just
because you didn't refer to it in the text doesn't mean some future reader
might not need to refer to it.  It is cumbersome to have to use
circumlocutions like ``the equation second from the top of page 3 column
1''.  (Note that the ruler will not be present in the final copy, so is not
an alternative to equation numbers).  All authors will benefit from reading
Mermin's description of how to write mathematics:
\url{http://www.pamitc.org/documents/mermin.pdf}.

Finally, you may feel you need to tell the reader that more details can be
found elsewhere, and refer them to a technical report.  For conference
submissions, the paper must stand on its own, and not {\em require} the
reviewer to go to a techreport for further details.  Thus, you may say in
the body of the paper ``further details may be found
in~\cite{Authors14b}''.  Then submit the techreport as additional material.
Again, you may not assume the reviewers will read this material.

Sometimes your paper is about a problem which you tested using a tool which
is widely known to be restricted to a single institution.  For example,
let's say it's 1969, you have solved a key problem on the Apollo lander,
and you believe that the CVPR70 audience would like to hear about your
solution.  The work is a development of your celebrated 1968 paper entitled
``Zero-g frobnication: How being the only people in the world with access to
the Apollo lander source code makes us a wow at parties'', by Zeus \etal.

You can handle this paper like any other.  Don't write ``We show how to
improve our previous work [Anonymous, 1968].  This time we tested the
algorithm on a lunar lander [name of lander removed for blind review]''.
That would be silly, and would immediately identify the authors. Instead
write the following:
\begin{quotation}
   \noindent
   We describe a system for zero-g frobnication.  This
   system is new because it handles the following cases:
   A, B.  Previous systems [Zeus et al. 1968] didn't
   handle case B properly.  Ours handles it by including
   a foo term in the bar integral.

   ...

   The proposed system was integrated with the Apollo
   lunar lander, and went all the way to the moon, don't
   you know.  It displayed the following behaviours
   which show how well we solved cases A and B: ...
\end{quotation}
As you can see, the above text follows standard scientific convention,
reads better than the first version, and does not explicitly name you as
the authors.  A reviewer might think it likely that the new paper was
written by Zeus \etal, but cannot make any decision based on that guess.
He or she would have to be sure that no other authors could have been
contracted to solve problem B.
\medskip

\noindent
FAQ\medskip\\
{\bf Q:} Are acknowledgements OK?\\
{\bf A:} No.  Leave them for the final copy.\medskip\\
{\bf Q:} How do I cite my results reported in open challenges?
{\bf A:} To conform with the double blind review policy, you can report results of other challenge participants together with your results in your paper. For your results, however, you should not identify yourself and should not mention your participation in the challenge. Instead present your results referring to the method proposed in your paper and draw conclusions based on the experimental comparison to other results.\medskip\\

\begin{figure}[t]
   \begin{center}
      \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
   \caption{Example of caption.  It is set in Roman so that mathematics
      (always set in Roman: $B \sin A = A \sin B$) may be included without an
      ugly clash.}
   \label{fig:long}
   \label{fig:onecol}
\end{figure}

\subsection{Miscellaneous}

\noindent
Compare the following:\\
\begin{tabular}{ll}
   \verb'$conf_a$' & $conf_a$          \\
   \verb'$\mathit{conf}_a$' & $\mathit{conf}_a$
\end{tabular}\\
See The \TeX book, p165.

The space after \eg, meaning ``for example'', should not be a
sentence-ending space. So \eg is correct, {\em e.g.} is not.  The provided
\verb'\eg' macro takes care of this.

When citing a multi-author paper, you may save space by using ``et alia'',
shortened to ``\etal'' (not ``{\em et.\ al.}'' as ``{\em et}'' is a complete word.)
However, use it only when there are three or more authors.  Thus, the
following is correct: ``
Frobnication has been trendy lately.
It was introduced by Alpher~\cite{Alpher02}, and subsequently developed by
Alpher and Fotheringham-Smythe~\cite{Alpher03}, and Alpher \etal~\cite{Alpher04}.''

This is incorrect: ``... subsequently developed by Alpher \etal~\cite{Alpher03} ...''
because reference~\cite{Alpher03} has just two authors.  If you use the
\verb'\etal' macro provided, then you need not worry about double periods
when used at the end of a sentence as in Alpher \etal.

For this citation style, keep multiple citations in numerical (not
chronological) order, so prefer \cite{Alpher03,Alpher02,Authors14} to
\cite{Alpher02,Alpher03,Authors14}.


\begin{figure*}
   \begin{center}
      \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \end{center}
   \caption{Example of a short caption, which should be centered.}
   \label{fig:short}
\end{figure*}

%------------------------------------------------------------------------
\subsection{Formatting your paper}

All text must be in a two-column format. The total allowable width of the
text area is $6\frac78$ inches (17.5 cm) wide by $8\frac78$ inches (22.54
cm) high. Columns are to be $3\frac14$ inches (8.25 cm) wide, with a
$\frac{5}{16}$ inch (0.8 cm) space between them. The main title (on the
first page) should begin 1.0 inch (2.54 cm) from the top edge of the
page. The second and following pages should begin 1.0 inch (2.54 cm) from
the top edge. On all pages, the bottom margin should be 1-1/8 inches (2.86
cm) from the bottom edge of the page for $8.5 \times 11$-inch paper; for A4
paper, approximately 1-5/8 inches (4.13 cm) from the bottom edge of the
page.

%-------------------------------------------------------------------------
\subsection{Margins and page numbering}

All printed material, including text, illustrations, and charts, must be kept
within a print area 6-7/8 inches (17.5 cm) wide by 8-7/8 inches (22.54 cm)
high.



%-------------------------------------------------------------------------
\subsection{Type-style and fonts}

Wherever Times is specified, Times Roman may also be used. If neither is
available on your word processor, please use the font closest in
appearance to Times to which you have access.

MAIN TITLE. Center the title 1-3/8 inches (3.49 cm) from the top edge of
the first page. The title should be in Times 14-point, boldface type.
Capitalize the first letter of nouns, pronouns, verbs, adjectives, and
adverbs; do not capitalize articles, coordinate conjunctions, or
prepositions (unless the title begins with such a word). Leave two blank
lines after the title.

AUTHOR NAME(s) and AFFILIATION(s) are to be centered beneath the title
and printed in Times 12-point, non-boldface type. This information is to
be followed by two blank lines.

The ABSTRACT and MAIN TEXT are to be in a two-column format.

MAIN TEXT. Type main text in 10-point Times, single-spaced. Do NOT use
double-spacing. All paragraphs should be indented 1 pica (approx. 1/6
inch or 0.422 cm). Make sure your text is fully justified---that is,
flush left and flush right. Please do not place any additional blank
lines between paragraphs.

Figure and table captions should be 9-point Roman type as in
Figures~\ref{fig:onecol} and~\ref{fig:short}.  Short captions should be centred.

\noindent Callouts should be 9-point Helvetica, non-boldface type.
Initially capitalize only the first word of section titles and first-,
second-, and third-order headings.

FIRST-ORDER HEADINGS. (For example, {\large \bf 1. Introduction})
should be Times 12-point boldface, initially capitalized, flush left,
with one blank line before, and one blank line after.

SECOND-ORDER HEADINGS. (For example, { \bf 1.1. Database elements})
should be Times 11-point boldface, initially capitalized, flush left,
with one blank line before, and one after. If you require a third-order
heading (we discourage it), use 10-point Times, boldface, initially
capitalized, flush left, preceded by one blank line, followed by a period
and your text on the same line.

%-------------------------------------------------------------------------
\subsection{Footnotes}

Please use footnotes\footnote {This is what a footnote looks like.  It
   often distracts the reader from the main flow of the argument.} sparingly.
Indeed, try to avoid footnotes altogether and include necessary peripheral
observations in
the text (within parentheses, if you prefer, as in this sentence).  If you
wish to use a footnote, place it at the bottom of the column on the page on
which it is referenced. Use Times 8-point type, single-spaced.


%-------------------------------------------------------------------------
\subsection{References}

List and number all bibliographical references in 9-point Times,
single-spaced, at the end of your paper. When referenced in the text,
enclose the citation number in square brackets, for
example~\cite{Authors14}.  Where appropriate, include the name(s) of
editors of referenced books.

\begin{table}
   \begin{center}
      \begin{tabular}{|l|c|}
         \hline
         Method & Frobnability           \\
         \hline\hline
         Theirs & Frumpy                 \\
         Yours  & Frobbly                \\
         Ours   & Makes one's heart Frob \\
         \hline
      \end{tabular}
   \end{center}
   \caption{Results.   Ours is better.}
\end{table}

%-------------------------------------------------------------------------
\subsection{Illustrations, graphs, and photographs}

All graphics should be centered.  Please ensure that any point you wish to
make is resolvable in a printed copy of the paper.  Resize fonts in figures
to match the font in the body text, and choose line widths which render
effectively in print.  Many readers (and reviewers), even of an electronic
copy, will choose to print your paper in order to read it.  You cannot
insist that they do otherwise, and therefore must not assume that they can
zoom in to see tiny details on a graphic.

When placing figures in \LaTeX, it's almost always best to use
\verb+\includegraphics+, and to specify the  figure width as a multiple of
the line width as in the example below
   {\small\begin{verbatim}
   \usepackage[dvips]{graphicx} ...
   \includegraphics[width=0.8\linewidth]
                   {myfile.eps}
\end{verbatim}
   }


%-------------------------------------------------------------------------
\subsection{Color}

Please refer to the author guidelines on the CVPR 2020 web page for a discussion
of the use of color in your document.

%------------------------------------------------------------------------
\newpage
\begin{table*}
   \begin{center}
      \begin{tabular}{|p{3cm}|p{5.5cm}|p{8.5cm}|}
         \hline
         \textbf{Student Name} & \textbf{Contributed Aspects}                                                   & \textbf{Details}                                                                                                                                                                                                                                                                                    \\
         \hline\hline
         Andrew Rollings       & Dataset Pre-processing, Architecture Design, Implementation, HP tuning, Report & Came up with initial idea of using CycleGANs and Video Game images. Created dataset and associated pre-processing. Implemented PyTorch dataset loader compatible with CycleGAN Tuned GAN hyper-parameters. Updated metrics calculation and hyper-parameter tuning wrapper code. Helped with report. \\
         \hline
         Michael Townsend      & Implementation, HP tuning, Metrics, Report                                     & Implemented first pass of custom metrics calculations. Tuned GAN hyper-parameters. Designed and implemented code wrapper for hyper-parameter tuning. Helped with report.                                                                                                                            \\
         \hline
         Tyler Thurston        & Implementation, Metrics, HP tuning, Version Control, Report                    & Setup team in version control. Git Repository organization and maintenance. Came up with NES color palette metric. Graphing coding. Tuned GAN hyper-parameters. Overall planning and scheduling. Helped with report.                                                                                \\
         \hline
      \end{tabular}
   \end{center}
   \caption{Contributions of team members.}
   \label{tab:contributions}
\end{table*}


%-------------------------------------------------------------------------
\begin{algorithm*}
   \caption{Metric for evaluating conversion quality}
   \begin{algorithmic}
      \label{alg:alg1}

      \STATE $console\_palette \leftarrow$ target console color palette normalized RGB values
      \STATE $max\_rgb \leftarrow$ corners of RGB space for $(i, j, k) \in i=\{0,1\}, j=\{0,1\}, k=\{0,1\}$
      \STATE $m\_bias \leftarrow 1 / $number of colors in target console color palette
      \STATE $total\_score \leftarrow 0$
      \STATE $num\_images \leftarrow $number of candidate images      
      \STATE
      \FOR{each candidate image}
      \STATE $img\_rgb \leftarrow $ per pixel normalized RGB values
      \STATE $min\_dists \leftarrow $ min distance between $img\_rgb$ and $console\_palette$
      \STATE $max\_dists \leftarrow $ min distance between $img\_rgb$ and $max\_rgb$ values.
      \STATE $match\_pct \leftarrow \sum_{i=1..n}[ 1 $ if $(min\_dists_i < 1)$ else $0] / n$
      \STATE $bias\_factor \leftarrow max(0, min(1, m\_bias + (1 - m\_bias) \times match\_pct)$

      \STATE $img\_score \leftarrow (\sum_{i=1..n}(max\_dists) - \sum_{i=1..n}(min\_dists)) / \sum_{i=1..n}(max\_dists)$
      \STATE $total\_score \leftarrow total\_score + img\_score \times bias\_factor$

      \ENDFOR

      \RETURN $total\_score/num\_images$

   \end{algorithmic}
\end{algorithm*}
%-------------------------------------------------------------------------

{\small
\bibliographystyle{ieee_fullname}
\bibliography{egbib}
}

\end{document}
